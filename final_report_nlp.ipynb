{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Programming Languages using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "- Our goal is to create a classification model to predict programming languages using Readme files from GitHub repositories.\n",
    "    \n",
    "\n",
    "### Key Takeaways\n",
    "- Most common programming language found in our dataset is Javascript followed by Python\n",
    "- Some of the most common words in Readmes were found to be: 'file', 'end', 'class','use' and 'object'.\n",
    "- The length of the readme's varies by programming languages.\n",
    "- \n",
    "- Our best model used  to predict programming languages with % accuracy. This model outperformed my baseline score of % accuracy, so it has value.\n",
    "\n",
    "### Project Overview\n",
    "- Trello board used to identify the different tasks for this project. You can find the board <a href=\"https://trello.com/b/PddXdOTJ/nlp-project\">here</a>\n",
    "- Python scripts were used to acquire, prepare and explore the data\n",
    "- \n",
    "- Statistical analyses tested the following hypotheses:\n",
    "    1. \n",
    " \n",
    "### Data Dictionary\n",
    "The data dictionary detailing all variables utilized in this analyses can be found <a href=\"https://github.com/mariam-and-cindy/predicting-programming-languages/blob/main/README.md\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# import acquire as aq\n",
    "# import prepare as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Data\n",
    "\n",
    "Our data was scraped from over 100 GitHub repositories. We decided to use the list of most forked repos on GitHub <a href=\"https://github.com/search?o=desc&p={i}&q=stars%3A%3E1&s=forks&type=Repositories\">here</a> for our dataset. \n",
    "\n",
    "This list of repos was cached as a csv after acquisition and using our acquire script we pulled the username and title, language and readme contents of every repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
