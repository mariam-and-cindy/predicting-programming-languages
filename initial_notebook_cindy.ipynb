{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare as prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are using the function in acquire.py to get a repo.\n",
    "\n",
    "**Note : the repo name should not contain spaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was working with a small size data to create prepare and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_json_file = 'data2.json'\n",
    "# df_github = pd.read_json(repo_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_github.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acquire big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from csv (data prevously acquired)\n",
    "df = pd.read_csv('git_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this section we are going to clean data:\n",
    "- remove non english readmes\n",
    "- check missing values. remove rows\n",
    "- remove duplicated rows\n",
    "- in exploration I realize that jupyter notebook was under language and it should be pyton\n",
    "- get only the top n programming languages\n",
    "- use my function that clean, steem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove non english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nonenglish = [16,19,37,43,76,90,108,115,124,131,135,138,143,151,152,156,166,171,174,185,193,197,199,\n",
    "       205,208,211, 216,217,221,224,229,230,239,241,248,273,277,278,279,292,298,310,311,316,317,\n",
    "       324,328,329,330,336,341,352,357,365,366,372,374,379,385,390,399]\n",
    "df = df.drop(df.index[[index_nonenglish]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NoTe:** we install textblob , it was working and then stop working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### detect language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install :\n",
    "```\n",
    "$ pip install -U textblob\n",
    "$ python -m textblob.download_corpora\n",
    "```\n",
    "https://textblob.readthedocs.io/en/dev/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.readme_contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = 'house is blue'\n",
    "lang = TextBlob(text)\n",
    "print(lang.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in range (0, len(df)):\n",
    "#     text = df.readme_contents[n]\n",
    "#     lang = TextBlob(text)\n",
    "#     print(lang.detect_language())\n",
    "#     if lang.detect_language() != 'en':\n",
    "#         df =df.drop([n])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function\n",
    "def remove_nonenglish (df):\n",
    "    '''\n",
    "    takes in df and 1 column to check if the text is in englis if not that row is going to be remove\n",
    "    '''\n",
    "    for n in range (0, len(df)):\n",
    "        text = df.clean[n]\n",
    "        lang = TextBlob(text)\n",
    "        if lang.detect_language() != 'en':\n",
    "            df =df.drop([n])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check missing values using my function\n",
    "miss_val =prep.miss_dup_values(df)\n",
    "miss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:  we can see we have missing values in language , so I'm going to remove the rows that have missing values in language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing missing values\n",
    "df = df.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change jupyter notebook to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['language'].replace('Jupyter Notebook', 'Python', inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the top n programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def top_n_target(df,target,  n):\n",
    "    '''\n",
    "    takes in a df and target and give you the top n of you target\n",
    "    return a df with only the top n \n",
    "    '''\n",
    "    \n",
    "    #get the value counts of the target\n",
    "    targ =pd.DataFrame(df[[target]].value_counts())\\\n",
    "    .reset_index().rename(columns= {0:'cnt', 'index':target})\n",
    "    #get the top 5\n",
    "    topl= list(targ.loc[0:(n-1)].language.values)\n",
    "    #get new df with only the top n values of target\n",
    "    df= df[df.language.isin(topl)].reset_index(drop=True)\n",
    "    return  df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = top_n_target(df, 'language', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean, tokenized, stemming, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use my prepare function\n",
    "df =prep.prepare_data(df, 'readme_contents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean (string):\n",
    "    '''\n",
    "    takes in a string and lowercase everything, normalize unicode characters, replace anything that is not a letter,\n",
    "    number, whitespace or a single quote.\n",
    "    retunr a clean string\n",
    "    '''\n",
    "    \n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKC',string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "    string = re.sub(r\"[^a-z0-9\\s]\", '', string)\n",
    "    string = re.sub(r'\\w*http\\w*', '', string)\n",
    "    string = re.sub(r'\\w*github\\w*', '', string)\n",
    "    string = re.sub(r'\\w*html\\w*', '', string)\n",
    "    string = re.sub(r'\\w*gmail\\w*', '', string)\n",
    "    string = re.sub(r'\\w*\\n\\w*', '', string)\n",
    "\n",
    "\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.readme_contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "#basic_clean (df.readme_contents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN FUNCTION FOR PREPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mf (df,extra_words=[], exclude_words=[] ):\n",
    "    '''\n",
    "    takes in a df and all the rows with missing information, non English text,\n",
    "    and then clean, tokenize, stemming, lemmatize\n",
    "    '''\n",
    "    \n",
    "    # remove non english\n",
    "    index_nonenglish = [16,19,37,43,76,90,108,115,124,131,135,138,143,151,152,156,166,171,174,185,193,197,199,\n",
    "       205,208,211, 216,217,221,224,229,230,239,241,248,273,277,278,279,292,298,310,311,316,317,\n",
    "       324,328,329,330,336,341,352,357,365,366,372,374,379,385,390,399]\n",
    "\n",
    "    df = df.drop(df.index[[index_nonenglish]])\n",
    "    \n",
    "    \n",
    "    #remove duplicates \n",
    "    df =df.drop_duplicates()\n",
    "    \n",
    "    #removing missing values\n",
    "    df = df.dropna(axis=0).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    #removing texts that are not English\n",
    "    #df = remove_nonenglish(df)\n",
    "\n",
    "    #replace Jupyter notebook by python\n",
    "    df['language'].replace('Jupyter Notebook', 'Python', inplace=True )\n",
    "    \n",
    "        \n",
    "        \n",
    "    #get the top n languages\n",
    "    df = top_n_target(df, 'language', 5).reset_index(drop=True)\n",
    "    \n",
    "    #use my prepare function to  clean, tokenized, stemming, lemmatize\n",
    "    df =prep.prepare_data(df, 'readme_contents', extra_words= extra_words, exclude_words=exclude_words)\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from csv (data prevously acquired)\n",
    "df = pd.read_csv('git_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to inclife as a stopwords\n",
    "extra_words =  ['javajcript', 'python', 'java', 'c', 'typescript', 'file', \n",
    "                'test', 'install', 'file', 'img', 'library','code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_mf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-37f0b2c05496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_words\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_mf' is not defined"
     ]
    }
   ],
   "source": [
    "#prepare data\n",
    "df = prepare_mf(df, extra_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>C++</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n  &lt;img src=\"https://www....</td>\n",
       "      <td>div endtoend open source machine learning comp...</td>\n",
       "      <td>div endtoend open sourc machin learn comprehen...</td>\n",
       "      <td>div endtoend open source machine learning comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twbs/bootstrap</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;a href=\"https://getboot...</td>\n",
       "      <td>p altbootstrap logo width200 sleek intuitive p...</td>\n",
       "      <td>p altbootstrap logo width200 sleek intuit powe...</td>\n",
       "      <td>p altbootstrap logo width200 sleek intuitive p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pierian-Data/Complete-Python-3-Bootcamp</td>\n",
       "      <td>Python</td>\n",
       "      <td># Complete-Python-3-Bootcamp\\nCourse Files for...</td>\n",
       "      <td>files complete 3 bootcamp course pierian data 95</td>\n",
       "      <td>file complet 3 bootcamp cours pierian data 95</td>\n",
       "      <td>file complete 3 bootcamp course pierian data 95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nightscout/cgm-remote-monitor</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>Nightscout Web Monitor (a.k.a. cgm-remote-moni...</td>\n",
       "      <td>nightscout web monitor aka herokuherokuimghero...</td>\n",
       "      <td>nightscout web monitor aka herokuherokuimghero...</td>\n",
       "      <td>nightscout web monitor aka herokuherokuimghero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>opencv/opencv</td>\n",
       "      <td>C++</td>\n",
       "      <td>## OpenCV: Open Source Computer Vision Library...</td>\n",
       "      <td>opencv open source computer vision homepage co...</td>\n",
       "      <td>opencv open sourc comput vision homepag cours ...</td>\n",
       "      <td>opencv open source computer vision homepage co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      repo    language  \\\n",
       "0                    tensorflow/tensorflow         C++   \n",
       "1                           twbs/bootstrap  JavaScript   \n",
       "2  Pierian-Data/Complete-Python-3-Bootcamp      Python   \n",
       "3            nightscout/cgm-remote-monitor  JavaScript   \n",
       "4                            opencv/opencv         C++   \n",
       "\n",
       "                                     readme_contents  \\\n",
       "0  <div align=\"center\">\\n  <img src=\"https://www....   \n",
       "1  <p align=\"center\">\\n  <a href=\"https://getboot...   \n",
       "2  # Complete-Python-3-Bootcamp\\nCourse Files for...   \n",
       "3  Nightscout Web Monitor (a.k.a. cgm-remote-moni...   \n",
       "4  ## OpenCV: Open Source Computer Vision Library...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  div endtoend open source machine learning comp...   \n",
       "1  p altbootstrap logo width200 sleek intuitive p...   \n",
       "2   files complete 3 bootcamp course pierian data 95   \n",
       "3  nightscout web monitor aka herokuherokuimghero...   \n",
       "4  opencv open source computer vision homepage co...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  div endtoend open sourc machin learn comprehen...   \n",
       "1  p altbootstrap logo width200 sleek intuit powe...   \n",
       "2      file complet 3 bootcamp cours pierian data 95   \n",
       "3  nightscout web monitor aka herokuherokuimghero...   \n",
       "4  opencv open sourc comput vision homepag cours ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  div endtoend open source machine learning comp...  \n",
       "1  p altbootstrap logo width200 sleek intuitive p...  \n",
       "2    file complete 3 bootcamp course pierian data 95  \n",
       "3  nightscout web monitor aka herokuherokuimghero...  \n",
       "4  opencv open source computer vision homepage co...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete df -> (180, 6)\n",
      "train -> (100, 6)\n",
      "validate -> (44, 6)\n",
      "test -> (36, 6)\n"
     ]
    }
   ],
   "source": [
    "#split train, validate and test\n",
    "train, validate, test = prep.split_data(df, 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data that you have scraped. Here are some ideas for exploration:\n",
    "\n",
    "- What are the most common words in READMEs?\n",
    "- What does the distribution of IDFs look like for the most common words?\n",
    "- Does the length of the README vary by programming language?\n",
    "- Do different programming languages use a different number of unique words?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What are the most common words in READMEs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of languages\n",
    "list_lang = list(train.language.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joint all the text of each language\n",
    "join_text ={}\n",
    "for language in list_lang :\n",
    "    join_text[language] = prep.basic_clean(' '.join(train[train.language == language].lemmatized))\n",
    "\n",
    "#add allwords\n",
    "join_text['all_words']= prep.basic_clean(' '.join(train.lemmatized))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#check if we have all the languages\n",
    "join_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#join_text['Ruby']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Represent text as word frequencies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word_count df \n",
    "word_counts= pd.DataFrame()\n",
    "for lang in join_text:\n",
    "    word_counts[lang] = pd.Series(join_text[lang].split()).value_counts()\n",
    "word_counts= word_counts.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts.sort_values('all_words', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#vizualizacion all the words for any text\n",
    "def word_cloud (text):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = WordCloud(background_color='white', width=800, height=600).generate(text)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(join_text['all_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if I do only 30 words?\n",
    "top30_allwords =word_counts.sort_values('all_words', ascending=False)[['all_words']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top30_allwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud_top(df,column, n_top=50):\n",
    "    '''\n",
    "    takes in a df , column and a number of top words to show\n",
    "    '''\n",
    "    top_all =df.sort_values(column, ascending=False)[[column]].head(n_top)\n",
    "    word_cloud(' '.join(top_all.index))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud_top(word_counts, 'all_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 50 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for col in word_counts.columns:\n",
    "    print (f' ********************* Language : {col} *********************')\n",
    "    wordcloud_top(word_counts, col)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def ngrams_wordcloud (text_list, title_list, n=2):\n",
    "#    for i in  range (0, len(text_list)):\n",
    "key= 'Python'\n",
    "n=2\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(2,2,1)\n",
    "pd.Series(nltk.ngrams(join_text['Python'].split(), n=n)).value_counts().head(20).plot.barh()\n",
    "plt.title(f'Top 10 most common {key} ngrams where n={n}')\n",
    "plt.subplot(2,2,2)\n",
    "img = WordCloud(background_color='white', width=800, height=600).generate(join_text['Python'])\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'The most common {key} ngrams where n={n}')\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_all =df.sort_values(column, ascending=False)[[column]].head(n_top)\n",
    "word_cloud(' '.join(top_all.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What does the distribution of IDFs look like for the most common words?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # our 3 example documents\n",
    "# documents = {\n",
    "#     'news': 'Codeup announced last thursday that they just launched a new data science program. It is 18 weeks long.',\n",
    "#     'description': 'Codeup\\'s data science program teaches hands on skills using Python and pandas.',\n",
    "#     'context': 'Codeup\\'s data science program was created in response to a percieved lack of data science talent, and growing demand.'\n",
    "# }\n",
    "# print(documents)\n",
    "\n",
    "# print('\\nCleaning and lemmatizing...\\n')\n",
    "\n",
    "# documents = {topic: prep.lemmatize(prep.basic_clean(documents[topic])) for topic in documents}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # A simple way to calculate idf for demonstration. Note that this\n",
    "# # function relies on the globally defined documents variable.\n",
    "# def idf(word):\n",
    "#     n_occurences = sum([1 for doc in documents.values() if word in doc])\n",
    "#     return len(documents) / n_occurences\n",
    "\n",
    "# # Get a list of the unique words\n",
    "# unique_words = pd.Series(' '.join(documents.values()).split()).unique()\n",
    "\n",
    "# # put the unique words into a data frame\n",
    "# (pd.DataFrame(dict(word=unique_words))\n",
    "#  # calculate the idf for each word\n",
    "#  .assign(idf=lambda df: df.word.apply(idf))\n",
    "#  # sort the data for presentation purposes\n",
    "#  .set_index('word')\n",
    "#  .sort_values(by='idf', ascending=False)\n",
    "#  .head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Get a list of the unique words\n",
    "# pd.Series(' '.join(documents.values()).split()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(join_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#join_text.pop(\"all_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    n_occurences = sum([1 for doc in join_text.values() if word in doc])\n",
    "    return len(join_text) / n_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the unique words\n",
    "unique_words = pd.Series(' '.join(join_text.values()).split()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check a single word\n",
    "idf('build')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_occurences = sum([1 for doc in join_text.values() if 'build' in doc])\n",
    "len(join_text) / n_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  how many texts  the word is in \n",
    "n_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total of texts \n",
    "len(join_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the unique words into a data frame\n",
    "un_df =(pd.DataFrame(dict(word=unique_words))\n",
    "\n",
    " # calculate the idf for each word\n",
    ".assign(idf=lambda train: train.word.apply(idf))\n",
    " # sort the data for presentation purposes\n",
    " .set_index('word')\n",
    " .sort_values(by='idf', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "un_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#these are the words tht are only in one type of language text\n",
    "un_df[un_df.idf==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(un_df[un_df.idf==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(un_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see\n",
    "len(un_df[un_df.idf==5]) /len(un_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#almost 66 % of the unique words are in one language text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_df.groupby(idf).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the length of the README vary by programming language?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['readme_length'] = train.lemmatized.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('language').readme_length.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "train.readme_length.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['word_count'] = train.lemmatized.apply(prep.basic_clean).apply(str.split).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "sns.relplot(data=train, y='word_count', x='readme_length', hue='language')\n",
    "#plt.rc('figure',figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "sns.barplot(data=train, y='word_count', x='language')\n",
    "#plt.rc('figure',figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "sns.barplot(data=train, y='readme_length', x='language')\n",
    "#plt.rc('figure',figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,8))\n",
    "sns.boxplot(x='language',y ='word_count', data= train)\n",
    "plt.ylim(0, 2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('language').word_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('language').word_count.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do different programming languages use a different number of unique words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts.all_words ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts[word_counts.all_words ==1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the unique words per language\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_words={}\n",
    "# Get a list of the unique words\n",
    "for key in join_text:\n",
    "    uniq_words [key] =  list(pd.Series(join_text[key].split()).unique())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join_text['JavaScript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(uniq_words['JavaScript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vizualizacion most frequet words\n",
    "def word_cloud (text):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    img = WordCloud(background_color='white', width=800, height=600).generate(text)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in uniq_words:    \n",
    "    print(f'******************************UNIQUE WORDS FOR {key} ***************************** ')\n",
    "    word_cloud (' '.join(uniq_words[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_uniq_df={}\n",
    "for key in uniq_words:\n",
    "    len_uniq_df[key] = len(uniq_words[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(len_uniq_df, index= ['unique_words']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finish ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'statushttpsgithubcomh5bphtml5boilerplateworkflowsbuild20statusbadgesvghttpsgithubcomh5bphtml5boilerplateactionsqueryworkflow3a22buildstatus22branch3amaster licensehttpsimgshieldsiobadgelicensemitlightgreysvghttpsgithubcomh5bphtml5boilerplateblobmasterlicensetxt devdependency statushttpsdaviddmorgh5bphtml5boilerplatedevstatussvghttpsdaviddmorgh5bphtml5boilerplateinfodevdependencies npm downloadshttpsimgshieldsionpmdthtml5boilerplatesvghttpswwwnpmjscompackagehtml5boilerplate githubstarsimagehttpsimgshieldsiogithubstarsh5bphtml5boilerplatesvglabelgithub20starshttpsgithubcomh5bphtml5boilerplate html5 boilerplate professional frontend template building fast robust adaptable web apps site project product 10 year iterative development community knowledge impose specific development philosophy framework youre free architect code way want homepage httpshtml5boilerplatecomhttpshtml5boilerplatecom source httpsgithubcomh5bphtml5boilerplatehttpsgithubcomh5bphtml5boilerplate twitter h5bphttpstwittercomh5bp quick start choose one following option download latest stable  github github release html5boilerplatecomhttpshtml5boilerplatecom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.remove_stopwords(string, ['http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "regexp = r'\\w*github\\w*'\n",
    "\n",
    "re.sub(regexp, '', string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling  X ---->do not  include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire\n",
    "df1 = pd.read_csv('giturls.csv', index_col=0)\n",
    "\n",
    "\n",
    "#prepare data\n",
    "df1 =prep.prepare_mf(df1)\n",
    "\n",
    "df1.drop(columns= 'index', inplace = True)\n",
    "\n",
    "#select lemmatize\n",
    "df = df1.drop(columns = ['readme_contents', 'clean', 'stemmed'])\n",
    "\n",
    "#split train, validate and test\n",
    "train, validate, test = prep.split_data(df, 'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = prep.split_Xy (train, validate, test, 'language' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df to store scores\n",
    "score_df = pd.DataFrame(columns = ['model', 'train_score', 'validate_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = cv.fit_transform(X_train.lemmatized)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfs = tfidf.fit_transform(X_train.lemmatized)\n",
    "\n",
    "pd.DataFrame(tfidfs.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Ngrams\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_words2 = cv.fit_transform(X_train.lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bag_of_words2.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bimports for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, recall_score, plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_train_val (model, name_dataset1, X, y, name_dataset2,  X2, y2 , name):\n",
    "    '''\n",
    "    Take in a  model and compare the  performance metrics of  Train, Evaluate  (only 2).\n",
    "    model: the model that you want to compare\n",
    "    name_dataset1 : type :train, validate or  test. Select one, STRING\n",
    "    X: df test, validate or test\n",
    "    y: df test, validate or test\n",
    "    name_dataset2: type :train, validate or  test. Select one, STRING\n",
    "    X2: df2 test, validate or test\n",
    "    y2: df2 test, validate or test\n",
    "    name:model name\n",
    "    \n",
    "    Example:\n",
    "    compare_metrics(logit2,'Train',X_train, y_train,'Test', X_test, y_test, 'dec_tree')\n",
    "    '''\n",
    "    from IPython.display import display, display_html\n",
    "    if name_dataset1.lower() != \"train\" and name_dataset1.lower() != \"validate\" and name_dataset1.lower() != \"test\" :\n",
    "        return print(\"incorrect name\")\n",
    "    if name_dataset2.lower() != \"train\" and name_dataset2.lower() != \"validate\" and name_dataset2.lower() != \"test\" :\n",
    "        return print(\"incorrect name\")\n",
    "    #prediction\n",
    "    pred_1 = model.predict(X)\n",
    "    pred_2 = model.predict(X2)\n",
    "\n",
    "    #score = accuracy\n",
    "    acc_1 = model.score(X, y)\n",
    "    acc_2 = model.score(X2, y2)\n",
    "\n",
    "\n",
    "    #conf Matrix\n",
    "    #model 1\n",
    "\n",
    "    conf_1 = confusion_matrix(y, pred_1)\n",
    "    cf_1 =  pd.DataFrame ((confusion_matrix(y, pred_1 )),index = ['a_JavaScript','a_Python', 'a_Java', 'a_C++', 'a_TypeScript' ],\\\n",
    "    columns =['p_JavaScript','p_Python', 'p_Java', 'p_C++', 'p_TypeScript' ])\n",
    "    \n",
    "    #model2\n",
    "    conf_2 = confusion_matrix(y2, pred_2)\n",
    "    cf_2 =  pd.DataFrame ((confusion_matrix(y2, pred_2 )),index = ['a_JavaScript','a_Python', 'a_Java', 'a_C++', 'a_TypeScript' ],\\\n",
    "    columns =['p_JavaScript','p_Python', 'p_Java', 'p_C++', 'p_TypeScript' ])\n",
    "    #\n",
    "\n",
    "    #classification report\n",
    "    #model1\n",
    "    clas_rep_1 =pd.DataFrame(classification_report(y, pred_1, output_dict=True)).T\n",
    "    clas_rep_1.rename(index={'0': \"dead\", '1': \"survived\"}, inplace = True)\n",
    "\n",
    "    #model2\n",
    "    clas_rep_2 =pd.DataFrame(classification_report(y2, pred_2, output_dict=True)).T\n",
    "    clas_rep_2.rename(index={'0': \"dead\", '1': \"survived\"}, inplace = True)\n",
    "    print(f'''\n",
    "    ******    {name_dataset1}       ******                              ******     {name_dataset2}    ****** \n",
    "       Overall Accuracy:  {acc_1:.2%}              |                Overall Accuracy:  {acc_2:.2%}  \n",
    "                                                \n",
    "\n",
    "    ____________________________________________________________________________________________________________\n",
    "    ''')\n",
    "  \n",
    "    cf_1_styler = cf_1.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset1} Confusion Matrix')\n",
    "    cf_2_styler = cf_2.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset2} Confusion Matrix')\n",
    "    space = \"\\xa0\" * 25\n",
    "    display_html(cf_1_styler._repr_html_()+ space  + cf_2_styler._repr_html_(), raw=True)\n",
    "    print('''\n",
    "     \n",
    "    \n",
    "    Classification Report:\n",
    "    ''')\n",
    "    clas_rep_1_styler = clas_rep_1.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset1} Classification Report')\n",
    "    clas_rep_2_styler = clas_rep_2.style.set_table_attributes(\"style='display:inline'\").set_caption(f'{name_dataset2} Classification Report')\n",
    "    space = \"\\xa0\" * 45\n",
    "    display_html(clas_rep_1_styler._repr_html_()+ space  + clas_rep_2_styler._repr_html_(), raw=True)\n",
    "    \n",
    "    \n",
    "    metric_dic = {'model_name': name, \n",
    "          (name_dataset1 +'_score'): acc_1,\n",
    "           (name_dataset2 +'_score'): acc_2}\n",
    "    return metric_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score ( X_train, y_train, X_validate, y_validate, model, name):\n",
    "    #fit the model\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    #get score train & validate\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    val_score = model.score(X_validate, y_validate)\n",
    "    #store metrics\n",
    "    metric_dic = {'model_name': name, \n",
    "              'train_score': train_score,\n",
    "               'validate_score': val_score}\n",
    "\n",
    "    \n",
    "    return metric_dic, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_xy_split(X_data, y_data):\n",
    "    '''\n",
    "    This function  split data during  NLP.\n",
    "    X_data : cv = CountVectorizer()\n",
    "            X = cv.fit_transform(df.lemmatized)\n",
    "    y_data : target (df.target)\n",
    "    \n",
    "    Returns : X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "    \n",
    "    Example :\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = nlp_xy_split (X,y)\n",
    "    '''\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train_validate, X_test, y_train_validate, y_test = train_test_split(X_data, y_data, \n",
    "                                                                          stratify = y_data, \n",
    "                                                                          test_size=.2, random_state=123)\n",
    "    \n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(X_train_validate, y_train_validate, \n",
    "                                                                stratify = y_train_validate, \n",
    "                                                                test_size=.3, \n",
    "                                                                random_state=123)\n",
    "    \n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df.lemmatized)\n",
    "y = df.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = nlp_xy_split (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df to store metrics\n",
    "metric_df =pd.DataFrame(columns= ['model_name', 'train_score', 'validate_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree = DecisionTreeClassifier(max_depth=4)\n",
    "model = d_tree.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_train_val(model,'train',X_train, y_train,'validate', X_validate, y_validate, 'dec_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metrics\n",
    "metric_df = metric_df.append(res, ignore_index = True)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive_Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit\n",
    "nb= MultinomialNB(alpha= 0.5, fit_prior= False )\n",
    "model2 =nb.fit(X_train, y_train)\n",
    "\n",
    "model2.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_train_val(model2,'train',X_train, y_train,'validate', X_validate, y_validate, 'naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metrics\n",
    "metric_df = metric_df.append(res, ignore_index = True)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the Random Forest Model using all columns\n",
    "model_rf1 = RandomForestClassifier(min_samples_leaf=5, random_state=1349)\n",
    "# fit the thing\n",
    "model3 = model_rf1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_train_val(model3,'train',X_train, y_train,'validate', X_validate, y_validate, 'rand_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metrics\n",
    "metric_df = metric_df.append(res, ignore_index = True)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the best k value with all columns\n",
    "for k in range(1,100):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc_t = knn.score(X_train, y_train)\n",
    "    acc_v = knn.score(X_validate, y_validate )\n",
    "    if (acc_t - acc_v) < .05 :\n",
    "        print(f'Model: {k} when k is {k}')\n",
    "        print(f'training score: {knn.score(X_train, y_train):.2%}')\n",
    "        print(f'validate score: {knn.score(X_validate, y_validate):.2%}')\n",
    "        print('__________________________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=49)\n",
    "model4 = knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_train_val(model4,'train',X_train, y_train,'validate', X_validate, y_validate, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add metrics\n",
    "metric_df = metric_df.append(res, ignore_index = True)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr1 =LogisticRegression(C=1.0 , random_state=123)\n",
    "model5=model_lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_train_val(model5,'train',X_train, y_train,'validate', X_validate, y_validate, 'log_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add metrics\n",
    "metric_df = metric_df.append(res, ignore_index = True)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with Ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "X_ngram= cv.fit_transform(df.lemmatized)\n",
    "y_ngram = df.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validate, y_validate, X_test, y_test = nlp_xy_split (X_ngram,y_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_model (mod, X_train, y_train, X_validate, y_validate, model_name, metric_df):\n",
    "    '''\n",
    "    this function fit model , calculate metrics for train, validate  and return a df with score\n",
    "    Example:\n",
    "    metric_df = whole_model ((LogisticRegression(C=1.0 , random_state=123)), X_train, y_train, X_validate, y_validate, 'log_reg_ngrams', metric_df)\n",
    "    '''\n",
    "    #fit\n",
    "    modelr =mod.fit(X_train, y_train)\n",
    "    #calculate metrics\n",
    "    res = compare_train_val(modelr,'train',X_train, y_train,'validate', X_validate, y_validate, model_name)\n",
    "    metric_df = metric_df.append(res, ignore_index = True)\n",
    "    return metric_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = whole_model ((LogisticRegression(C=1.0 , random_state=123)), X_train, y_train, X_validate, y_validate, 'log_reg_ngrams', metric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_list = [DecisionTreeClassifier(max_depth=4),\n",
    "      MultinomialNB(alpha= 0.5, fit_prior= False ),\n",
    "      KNeighborsClassifier(n_neighbors=49),\n",
    "      RandomForestClassifier(min_samples_leaf=5, random_state=1349),\n",
    "      LogisticRegression(C=1.0 , random_state=123)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_name_list = ['dec_tree', 'nb', 'knn', 'ran_forest', 'log_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for mod, mod_names in zip(mod_list, mod_name_list):\n",
    "    print (f'******************************** MODEL: {mod}*******************************')\n",
    "    metric_df = whole_model (mod, X_train, y_train, X_validate, y_validate, mod_names, metric_df)\n",
    "    print(' ')\n",
    "    print(' _______________________________________________________________________________________________________')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
